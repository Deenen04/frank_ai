import os
import json
import base64
# import time # Not explicitly used for time.sleep, asyncio.sleep is used
import asyncio
import logging
import re
from typing import List, Optional
import langid  # language identification library
langid.set_languages(["en", "fr", "de"])

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, Request, Form
from fastapi.responses import HTMLResponse
from twilio.twiml.voice_response import VoiceResponse, Connect, Stream, Parameter # Parameter not used in current code
from dotenv import load_dotenv
from starlette.websockets import WebSocketState # Import for checking WebSocket state


from whisper_streamer import WhisperStreamer as DeepgramStreamer
from elevenlabs.client import ElevenLabs
# Streaming generation helper
from apiChatCompletion import make_openai_request
# Decision prompt template is imported below; we build it with simple replace.
# Import languageâ€specific prompt templates
from ai_prompt import build_prompt
# (generate_reply is kept for non-streaming fallbacks if needed)
from ai_executor import generate_reply  # Import the real AI function (fallback)
# from route_call import route_call # Assuming this is defined elsewhere

# ------------------------------------------------------------------
# Ultra-fast heuristic language detector (English / French / German)
# ------------------------------------------------------------------
_FR_CHARS = set("Ã©Ã¨ÃªÃ«Ã Ã¢Ã¤Ã®Ã¯Ã´Ã¶Ã¹Ã»Ã¼Ã§Å“")
_DE_CHARS = set("Ã¤Ã¶Ã¼ÃŸ")


def fast_lang_detect(text: str) -> str:
    """Return "fr", "de" or "en" using a cheap character-based heuristic.

    The heuristic looks for any accented characters that are unique to
    French or German. If none are found we default to English.
    This runs in O(len(text)) time and on short strings typically takes
    <0.01 ms â€“ well below the 0.01 s requirement.
    """
    lowered = text.lower()
    if any(ch in _FR_CHARS for ch in lowered):
        return "fr"
    if any(ch in _DE_CHARS for ch in lowered):
        return "de"
    return "en"

def route_call(call_sid):
    log.info(f"Mock route_call called for SID: {call_sid}")


load_dotenv()
HOSTNAME = os.getenv("HOSTNAME_twilio", "localhost:8000") # Ensure port if uvicorn runs on non-80
DEEPGRAM_API_KEY = os.getenv("DEEPGRAM_API_KEY")  # optional now â€“ kept for backwards compat
ELEVEN_API_KEY = os.getenv("ELEVEN_API_KEY")

# Local Whisper no longer needs the Deepgram key â€“ do not error if missing.

logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)-7s | %(name)s | %(message)s")
log = logging.getLogger("voicebot")

# Enable DEBUG logging for Deepgram VAD debugging
deepgram_log = logging.getLogger("deepgram_streamer")
deepgram_log.setLevel(logging.DEBUG)

VOICE_IDS = {"en": "kdnRe2koJdOK4Ovxn2DI", "fr": "OYTbf65OHHFELVut7v2H", "de": "v3V1d2rk6528UrLKRuy8"}
FAREWELL_LINES = {"en": "Thanks for calling. Goodbye.", "fr": "Merci d'avoir appelÃ©. Au revoir.", "de": "Danke fÃ¼r Ihren Anruf. Auf Wiedersehen."}
GREETING_LINES = {"en": "Hi, This is Frank Babar Clinic, I am here to assist you book an appointment with us today. How can I help you?", "fr": "Bonjour, comment puis-je vous aider?", "de": "Hallo, wie kann ich Ihnen helfen?"}
END_DELAY_SEC = 1 # Reduced for faster testing
USER_TURN_END_DELAY_SEC = 0.8 # Reduced for faster response

class TranscriptSanitizer:
    def __init__(self):
        self.reset()

    def reset(self):
        self._final_parts: List[str] = []
        self._last_interim: str = ""

    def add_transcript(self, new_transcript: str, is_final: bool = False):
        """Add a new transcript part, properly handling interim vs final results."""
        if not new_transcript.strip():
            return
        
        # Handle based on whether this is a final or interim result
        if is_final:
            # This is a final result from Deepgram ASR
            # Add any existing interim to final parts first
            if self._last_interim:
                self._final_parts.append(self._last_interim)
                self._last_interim = ""
            # Add the final transcript
            self._final_parts.append(new_transcript)
        else:
            # This is an interim result - replace the last interim
            # Deepgram interim results usually replace previous interims for the same utterance
            self._last_interim = new_transcript

    def get_current_transcript(self) -> str:
        """Get the current complete transcript including final parts and current interim."""
        return " ".join(self._final_parts + ([self._last_interim] if self._last_interim else [])).strip()

    def finalize_utterance(self) -> str:
        """Finalize the current utterance and reset for next one."""
        if self._last_interim:
            self._final_parts.append(self._last_interim)
            self._last_interim = ""
        full_transcript = " ".join(self._final_parts)
        self._final_parts = []  # Reset for next full utterance
        return " ".join(full_transcript.split())  # Normalize spaces


class TTSController:
    def __init__(self):
        # Active ElevenLabs generator instance (None when idle)
        self.current_generator = None
        # Whether the TTS engine is currently streaming audio to the client
        self.is_speaking = False
        # Accumulates plain-text chunks that have been **fully** spoken to the caller.
        # This allows the application to know exactly what words were delivered even
        # if the audio was interrupted mid-sentence.
        self.spoken_text_parts: list[str] = []

    # ------------------------------------------------------------------
    # Utility helpers for spoken-text accounting
    # ------------------------------------------------------------------

    def reset_spoken_text(self):
        """Clear the list of text chunks that were already streamed."""
        self.spoken_text_parts.clear()

    def add_spoken_text(self, text: str):
        """Record a *text* chunk that has been sent completely to the caller."""
        if text:
            self.spoken_text_parts.append(text.strip())

    def get_spoken_text(self) -> str:
        """Return the concatenated text that has been spoken so far."""
        return " ".join(self.spoken_text_parts).strip()

    def stop_immediately(self):
        if self.current_generator:
            log.debug("[TTS] Attempting to stop TTS generator.")
            try: self.current_generator.close() # For generator-based streams
            except GeneratorExit:
                log.debug("[TTS] GeneratorExit caught.")
            except Exception as e:
                log.warning(f"[TTS] Exception closing generator: {e}")
            self.current_generator = None
        self.is_speaking = False

app = FastAPI()
tts_client = ElevenLabs(api_key=ELEVEN_API_KEY)

@app.on_event("startup")
async def startup_event(): # Renamed to avoid conflict with `startup` variable if any
    log.info("[STARTUP] Voice agent started â€” integrated DeepgramStreamer")

@app.post("/voice", response_class=HTMLResponse)
async def answer_call(_request: Request, From: str = Form(...), To: str = Form(...)): # Renamed `answer`
    log.info("[/voice] %s âž” %s", From, To)
    vr = VoiceResponse()
    connect = Connect() # Corrected variable name
    # Ensure HOSTNAME includes scheme for wss if it's not just the host
    ws_url = f"wss://{HOSTNAME}/media" if not HOSTNAME.startswith("localhost") else f"ws://{HOSTNAME}/media"
    
    # Check if HOSTNAME from .env already has ws:// or wss://
    if "://" in HOSTNAME:
        ws_url = f"{HOSTNAME}/media" # Assume full URL like wss://myhost.com or ws://localhost:8000
    elif ":" in HOSTNAME: # e.g. localhost:8000
        ws_url = f"ws://{HOSTNAME}/media"
    else: # e.g. mydomain.com
        ws_url = f"wss://{HOSTNAME}/media"


    log.info(f"Connecting to WebSocket: {ws_url}")
    stream = Stream(url=ws_url)
    # Twilio sends these as custom parameters in the 'start' message
    stream.parameter(name="caller_phone_number", value=From) # Parameter class has lowercase 'p'
    stream.parameter(name="brand_phone_number", value=To)
    # CallSid is available as a template variable in TwiML
    stream.parameter(name="twilio_call_sid", value="{{CallSid}}")
    connect.append(stream)
    vr.append(connect)
    return HTMLResponse(str(vr), media_type="application/xml")

async def play_greeting(lang: str, sid: str, ws: WebSocket, tts_controller: TTSController, state: dict):
    voice_id, text = VOICE_IDS.get(lang, VOICE_IDS["en"]), GREETING_LINES.get(lang, GREETING_LINES["en"])
    
    # state["user_is_speaking"] = False # This should be controlled by Deepgram events
    tts_controller.is_speaking = True
    log.info(f"[GREETING-{sid}] Playing: '{text}'")
    try:
        tts_controller.current_generator = tts_client.text_to_speech.stream(
            text=text, voice_id=voice_id, model_id="eleven_flash_v2_5", # eleven_flash_v2_5 is not a public model name. Use eleven_multilingual_v2 or eleven_turbo_v2
            output_format="ulaw_8000", optimize_streaming_latency=0
        )
        for audio_chunk_count, audio in enumerate(tts_controller.current_generator):
            if state.get("user_is_speaking"):
                log.warning(f"[GREETING-CUTOFF-{sid}] User started speaking. Stopping greeting.")
                tts_controller.stop_immediately() # This will break out of the loop
                break
            if state["stop_call"]: # Renamed for clarity
                log.info(f"[GREETING-STOP-{sid}] Call stop requested. Stopping greeting.")
                tts_controller.stop_immediately()
                break
            if ws.client_state != WebSocketState.CONNECTED:
                log.warning(f"[GREETING-DISCONNECTED-{sid}] WebSocket no longer connected. Stopping TTS.")
                tts_controller.stop_immediately()
                break
            
            # log.debug(f"[GREETING-{sid}] Sending audio chunk {audio_chunk_count}")
            await ws.send_text(json.dumps({
                "event": "media", "streamSid": sid,
                "media": {"payload": base64.b64encode(audio).decode()}
            }))
            # Yield control to the event loop without introducing a fixed delay
            await asyncio.sleep(0)

    except Exception as e:
        log.warning(f"[GREETING-ERROR-{sid}] TTS streaming interrupted: {e}", exc_info=True)
    finally:
        log.debug(f"[GREETING-FINALLY-{sid}] Cleaning up TTS.")
        tts_controller.is_speaking = False # Ensure this is reset
        # current_generator is set to None in stop_immediately

    if not state["stop_call"] and ws.client_state == WebSocketState.CONNECTED:
        log.info(f"[GREETING-MARK-{sid}] Sending end_of_ai_turn mark.")
        try:
            await ws.send_text(json.dumps({"event": "mark", "streamSid": sid,
                                           "mark": {"name": "end_of_ai_turn"}}))
        except RuntimeError as e: # Catch specific error for sending on closed socket
            if "WebSocket is not connected" in str(e) or "after sending 'websocket.close'" in str(e):
                log.warning(f"[GREETING-MARK-ERROR-{sid}] WebSocket already closed when trying to send mark: {e}")
            else:
                log.error(f"[GREETING-MARK-ERROR-{sid}] Unexpected RuntimeError: {e}", exc_info=True)
            # raise # Optionally re-raise if it's truly unexpected
        except Exception as e:
            log.error(f"[GREETING-MARK-ERROR-{sid}] Failed to send mark: {e}", exc_info=True)
    else:
        log.info(f"[GREETING-MARK-SKIP-{sid}] Skipping end_of_ai_turn mark (stop_call={state['stop_call']}, ws_state={ws.client_state}).")


def display_live_transcript(caller: str, transcript: str):
    print(f"\r[LIVE] {caller or 'Caller'}: {transcript}", end="", flush=True)

def display_final_turn(caller: str, full_phrase: str):
    print(f"\n[TURN] {caller or 'Caller'}: {full_phrase}")
    print("-" * 60, flush=True)

@app.websocket("/media")
async def media_websocket_endpoint(ws: WebSocket): # Renamed `media`
    await ws.accept()
    log.info("[/media] WebSocket connection accepted.")
    
    # State and helpers
    # Use more descriptive state keys
    call_state = {
        "stop_call": False, 
        "initiate_transfer": False, # Was "route_call"
        "terminate_call": False,    # Was "end_call"
        "user_is_speaking": False,
        "twilio_stream_sid": None,
        "caller_phone_number": "?",
        "brand_phone_number": "?",
        "twilio_call_sid": None, # From Twilio custom parameters
        "last_processed_transcript": "",  # Track what transcript we've already processed
        "pending_transcript_parts": [],   # Store partial transcripts for aggregation
    }
    tts_controller = TTSController()
    # DeepgramStreamer now emits complete utterances, so we no longer need TranscriptSanitizer.
    current_language = "multi" # Default language
    conversation_history: List[str] = []
    
    # Task management
    ai_response_task: Optional[asyncio.Task] = None
    # user_turn_end_timer: Optional[asyncio.Task] = None # No longer needed with on_utterance callback
    action_watcher_task: Optional[asyncio.Task] = None


    deepgram_streamer = DeepgramStreamer( # Renamed from streamer
        api_key=DEEPGRAM_API_KEY,
        encoding="mulaw", # Twilio default is mulaw
        sample_rate=8000, # Twilio default is 8000Hz
        interim_results=False,
        vad_events=True,  # Keep VAD for fallback even though endpointing is used
        punctuate=True,
        model='groq-whisper-large-v3-turbo',  # Use groq-whisper-large-v3-turbo
        language=current_language, # Start with default, can be changed
        # Enhanced amplitude-based VAD parameters
        use_amplitude_vad=True,
        amplitude_threshold_db=-5.0,  # This translates to 700 RMS - more reliable speech detection
        silence_timeout=2.0,  # Longer timeout for natural speech patterns
        # Increase the interval between partial transcriptions to reduce
        # Whisper invocations (still responsive on GPU)
        partial_interval=0.7
    )

    async def process_final_utterance(final_utterance: str):
        """Handle a complete user utterance coming from DeepgramStreamer."""
        nonlocal ai_response_task, current_language
        
        # --------------------------------------------------------------
        # Opportunistic language detection â€“ allow switching to FR/DE/EN
        # at any point once the confidence is high enough (â‰¥ 0.80).
        # We no longer *force* the language to EN on unknown input so that
        # a later utterance in FR/DE can still change the language.
        # --------------------------------------------------------------
        if final_utterance:
            lang_code, lang_conf = langid.classify(final_utterance)

            # We only act when we are reasonably confident (>40%).
            if lang_conf >= 0.40 and lang_code in ("fr", "de", "en"):
                if lang_code != current_language:
                    prev_lang = current_language
                    current_language = lang_code
                    log.info(
                        f"[LANG-DETECT] Switching language {prev_lang.upper()} âž” {lang_code.upper()} "
                        f"(conf={lang_conf:.2f})"
                    )
            elif current_language == "multi":
                # Still no confident detection â†’ stay in neutral state.
                log.debug(
                    f"[LANG-DETECT] Low-confidence ({lang_code}, {lang_conf:.2f}) â€“ keeping language 'multi'."
                )
        
        # Handle empty transcripts (initial VAD detection)
        if not final_utterance:
            log.info("[PROCESS_UTTERANCE] Received empty utterance - VAD detected speech end, waiting for transcript...")
            # Don't process empty utterances, but mark user as not speaking
            call_state["user_is_speaking"] = False
            # Cancel any ongoing AI if user was detected speaking
            if tts_controller.is_speaking:
                log.info("[PROCESS_UTTERANCE] Stopping AI audio due to speech detection.")
                tts_controller.stop_immediately()
            return

        # Check if this is an aggregated transcript (contains previous parts)
        if call_state["last_processed_transcript"] and call_state["last_processed_transcript"] in final_utterance:
            log.info(f"[PROCESS_UTTERANCE] Received aggregated transcript: '{final_utterance}' (previous: '{call_state['last_processed_transcript']}')")
            # This is an aggregated transcript, interrupt any current AI and process the complete version
            if tts_controller.is_speaking:
                log.info("[PROCESS_UTTERANCE] Interrupting AI for aggregated transcript.")
                tts_controller.stop_immediately()
            if ai_response_task and not ai_response_task.done():
                log.warning("[PROCESS_UTTERANCE] Cancelling previous AI task for aggregated transcript.")
                ai_response_task.cancel()
            
            # Remove the old entry from conversation history if it exists
            if call_state["last_processed_transcript"]:
                for i in range(len(conversation_history) - 1, -1, -1):
                    if conversation_history[i].startswith("Human: ") and call_state["last_processed_transcript"] in conversation_history[i]:
                        log.info(f"[PROCESS_UTTERANCE] Removing old conversation entry: {conversation_history[i]}")
                        conversation_history.pop(i)
                        break
        else:
            log.info(f"[PROCESS_UTTERANCE] Processing new transcript: '{final_utterance}'")

        # Mark that user finished speaking
        call_state["user_is_speaking"] = False
        call_state["last_processed_transcript"] = final_utterance

        display_final_turn(call_state["caller_phone_number"], final_utterance)

        # Cancel any ongoing AI generation / TTS (in case not already done above)
        if ai_response_task and not ai_response_task.done():
            log.warning("[PROCESS_UTTERANCE] Cancelling previous AI task â€“ user spoke over AI.")
            ai_response_task.cancel()
        tts_controller.stop_immediately()

        # Add to conversation history
        conversation_history.append(f"Human: {final_utterance}")

        # Launch AI response
        ai_response_task = asyncio.create_task(
            handle_ai_turn(call_state, current_language, ws, conversation_history, tts_controller)
        )

        # In process_final_utterance, after stripping final_utterance (we removed earlier), add duplicate check
        if final_utterance.lower() == call_state.get("last_processed_transcript", "").lower():
            log.info("[PROCESS_UTTERANCE] Duplicate utterance detected â€” ignoring.")
            return

    def on_dg_speech_start():
        # Pure VAD signal â€“ we now wait for actual transcript before interrupting TTS.
        # Leaving this as a debug notification only.
        if call_state["stop_call"]:
            return
        log.debug("[DG] Speech start detected (waiting for transcript before taking action).")

    MIN_TRANSCRIPT_CONFIDENCE = 30.0  # percent

    def on_dg_transcript(*args):
        """Handle interim transcripts (Deepgram or Whisper partials).

        Accepts both the 2-argument signature (*transcript*, *is_final*) used by
        DeepgramStreamer **and** the 3-argument signature (*transcript*,
        *is_final*, *confidence*) emitted by the enhanced WhisperStreamer.
        """
        if call_state["stop_call"]:
            return

        # ------------------------------------------------------------------
        # Parse *args* â†’ transcript, is_final, confidence
        # ------------------------------------------------------------------
        transcript = args[0] if args else ""
        is_final = args[1] if len(args) >= 2 else False
        confidence = args[2] if len(args) >= 3 else 100.0  # Deepgram has no conf

        if not transcript.strip():
            return

        # Mark that caller is speaking (confirmed by ASR text)
        call_state["user_is_speaking"] = True

        # Only interrupt TTS when the transcript meets the confidence threshold
        if confidence >= MIN_TRANSCRIPT_CONFIDENCE and tts_controller.is_speaking:
            log.info(
                f"[DG-TRANSCRIPT] Stopping TTS â€“ transcript='{transcript[:60]}â€¦', final={is_final}, conf={confidence:.1f}%"
            )
            tts_controller.stop_immediately()

        # Display live transcript for debugging
        display_live_transcript(call_state["caller_phone_number"], transcript)

    def on_dg_speech_end():
        # Optional: this is already covered by on_utterance, but good for state tracking
        if call_state["stop_call"]:
            return
        call_state["user_is_speaking"] = False
        log.info("[DG] Speech end detected.")

    MIN_WHISPER_CONFIDENCE = 35.0  # percent â€“ only process utterances above this threshold
    
    def on_dg_utterance(*args):
        """Handle complete utterances emitted by the ASR stack.

        The *WhisperStreamer* now forwards a *confidence* score (0-100) as the
        second positional argument.  For backwards-compatibility we accept both
        the original 1-argument as well as the new 2-argument signature.
        """
        nonlocal current_language
        if call_state["stop_call"]:
            return

        # Parse *args* â†’ (utterance, confidence[, detected_lang])
        if len(args) == 1:
            utterance = args[0]
            confidence = 100.0  # assume perfect confidence when value missing (e.g. Deepgram backend)
            detected_lang = None
        elif len(args) == 2:
            utterance, confidence = args[:2]
            detected_lang = None
        else:
            utterance, confidence, detected_lang = args[:3]

        # Log confidence and language for debugging
        log.info(
            "[UTTERANCE] Confidence %.1f%% â€” '%s' (lang=%s)",
            confidence,
            utterance,
            detected_lang,
        )

        # Ignore trivial "thank you" style utterances entirely
        if _is_trivial_thanks(utterance):
            log.debug("[UTTERANCE] Ignoring trivial thank-you phrase.")
            return

        # If Whisper provided a language (en/fr/de), prefer it over langid
        if detected_lang in ("en", "fr", "de") and detected_lang != current_language:
            log.info(
                "[LANG-DETECT] Whisper suggests switch %s âž” %s",
                current_language.upper(),
                detected_lang.upper(),
            )
            current_language = detected_lang

        # Discard low-confidence utterances
        if confidence < MIN_WHISPER_CONFIDENCE:
            log.warning("[UTTERANCE] Ignoring low-confidence transcript (%.1f%% < %.1f)", confidence, MIN_WHISPER_CONFIDENCE)
            return

        # This is triggered by the enhanced VAD system:
        # 1. VAD detects speech end and sends transcript immediately (even if empty)
        # 2. If more transcript parts arrive later, they are aggregated and sent again
        # 3. The main processing handles interruption of current AI responses
        asyncio.create_task(process_final_utterance(utterance))

    # Register callbacks with the streamer
    deepgram_streamer.on('on_transcript', on_dg_transcript)  # Add transcript callback for hard stop
    deepgram_streamer.on('on_speech_start', on_dg_speech_start)
    deepgram_streamer.on('on_speech_end', on_dg_speech_end)
    deepgram_streamer.on('on_utterance', on_dg_utterance)

    try:
        log.info("[/media] Connecting to Deepgram...")
        await deepgram_streamer.connect()
        log.info("[/media] Deepgram connection successful.")
    except Exception as e:
        log.error(f"[/media] Failed to connect to Deepgram: {e}", exc_info=True)
        # Send error response to client and close
        if ws.client_state == WebSocketState.CONNECTED:
            await ws.close(code=1011, reason="Failed to connect to speech service")
        return

    try:
        # Watch for routing/end signals - this can be simplified or integrated
        async def call_action_watcher():
            log.debug("[ACTION_WATCHER] Started.")
            while not call_state["stop_call"]: # Loop as long as call is active
                if call_state["initiate_transfer"] and call_state["twilio_call_sid"] and '{{' not in call_state["twilio_call_sid"]:
                    log.info(f"[/media] Routing call {call_state['twilio_call_sid']}")
                    # await asyncio.get_event_loop().run_in_executor(None, route_call, call_state["twilio_call_sid"])
                    route_call(call_state["twilio_call_sid"]) # Assuming route_call is not IO-bound blocking
                    call_state["stop_call"] = True # Mark to stop processing
                    break 
                if call_state["terminate_call"]:
                    log.info(f"[/media] Terminating call {call_state['twilio_call_sid']}")
                    call_state["stop_call"] = True # Mark to stop processing
                    break
                await asyncio.sleep(0.1)
            log.debug("[ACTION_WATCHER] Exited.")
            # Ensure WebSocket is closed if watcher decides to stop the call
            if ws.client_state == WebSocketState.CONNECTED:
                log.info("[ACTION_WATCHER] Closing WebSocket due to call action.")
                await ws.close(code=1000)


        action_watcher_task = asyncio.create_task(call_action_watcher())

        # Main receive loop for messages from Twilio
        while not call_state["stop_call"] and ws.client_state == WebSocketState.CONNECTED:
            try:
                raw_message = await ws.receive_text()
            except WebSocketDisconnect:
                log.warning("[/media] WebSocket disconnected by client (Twilio).")
                call_state["stop_call"] = True
                break # Exit main loop

            message = json.loads(raw_message)
            event_type = message.get("event")

            if event_type == "start":
                call_state["twilio_stream_sid"] = message["start"]["streamSid"]
                custom_params = message["start"].get("customParameters", {})
                call_state["caller_phone_number"] = custom_params.get("caller_phone_number", "?")
                call_state["brand_phone_number"] = custom_params.get("brand_phone_number", "?")
                call_state["twilio_call_sid"] = custom_params.get("twilio_call_sid") # Store this
                
                log.info(f"[WS-START] SID: {call_state['twilio_stream_sid']}, "
                         f"From: {call_state['caller_phone_number']}, To: {call_state['brand_phone_number']}, "
                         f"CallSID: {call_state['twilio_call_sid']}")
                print(f"\nðŸŽ¯ CALL STARTED: {call_state['caller_phone_number']} â†’ {call_state['brand_phone_number']}")
                print("="*60)
                
                # Start greeting
                asyncio.create_task(play_greeting(
                    current_language, call_state["twilio_stream_sid"], ws, tts_controller, call_state
                ))

            elif event_type == "media":
                if call_state["stop_call"] or not deepgram_streamer.is_open():  # DG connection check
                    continue 
                
                payload = base64.b64decode(message["media"]["payload"])
                try:
                    await deepgram_streamer.send(payload)
                except Exception as e:
                    log.warning(f"[/media] Error sending audio to Deepgram: {e}")
                    # Continue processing, don't break the loop for individual send errors

            elif event_type == "mark":
                mark_name = message.get("mark", {}).get("name")
                sid_logged = message.get("streamSid") or call_state.get("twilio_stream_sid", "?")
                log.info(f"[WS-MARK] Received mark: {mark_name} for SID {sid_logged}")
                if mark_name == "end_of_ai_turn":
                    # This confirms AI has finished speaking its part.
                    # Useful if you need to take action after AI speaks.
                    # Currently, user can interrupt, so this might not always be the true end of AI's turn.
                    pass


            elif event_type == "stop":
                stop_obj = message.get("stop", {}) if isinstance(message, dict) else {}
                sid_logged = stop_obj.get("streamSid") or message.get("streamSid") or call_state.get("twilio_stream_sid", "?")
                log.info(f"[WS-STOP] Received stop event from Twilio for SID {sid_logged}. Call is ending.")
                call_state["stop_call"] = True
                break # Exit main loop
            
            elif event_type == "dtmf":
                log.info(f"[WS-DTMF] Received DTMF digit: {message['dtmf']['digit']} for SID {message['streamSid']}")
                # Handle DTMF if needed, e.g., to interrupt AI or trigger actions

            else:
                log.warning(f"[/media] Received unknown event type: {event_type}")
                log.debug(f"Unknown message: {message}")

    except WebSocketDisconnect:
        log.warning("[/media] WebSocket disconnected unexpectedly during processing.")
    except Exception as e:
        log.error(f"[/media] Error in WebSocket handler: {e}", exc_info=True)
    finally:
        log.info(f"[/media] Cleaning up for call {call_state.get('twilio_call_sid', 'N/A')}")
        call_state["stop_call"] = True # Ensure flag is set for all cleanup tasks

        # Cancel all pending tasks
        if ai_response_task and not ai_response_task.done():
            log.debug("[/media] Cancelling AI response task.")
            ai_response_task.cancel()
        if action_watcher_task and not action_watcher_task.done():
            log.debug("[/media] Cancelling action watcher task.")
            action_watcher_task.cancel()
        
        # Stop any active TTS
        tts_controller.stop_immediately()

        # Clean up Deepgram connection
        if deepgram_streamer:
            log.info("[/media] Finishing Deepgram streamer.")
            try:
                await deepgram_streamer.finish()
            except Exception as e:
                log.error(f"[/media] Error finishing Deepgram streamer: {e}", exc_info=True)

        # Ensure WebSocket is closed if not already
        if ws.client_state == WebSocketState.CONNECTED:
            log.info("[/media] Closing WebSocket connection in finally block.")
            try:
                await ws.close(code=1000)
            except Exception as e:
                log.error(f"[/media] Error closing WebSocket in finally: {e}")
        
        print(f"\nðŸ”š CALL ENDED: {call_state.get('caller_phone_number', 'N/A')}")
        print("="*60, flush=True)
        log.info("[/media] WebSocket cleanup complete.")


async def handle_ai_turn(call_state: dict, lang: str, ws: WebSocket,
                         conversation_history: List[str], tts_controller: TTSController):
    
    lang_selected = lang  # mutable copy â€“ may switch after first AI words
    voice_id = VOICE_IDS.get(lang_selected, VOICE_IDS["en"])
    stream_sid = call_state["twilio_stream_sid"]  # cached for quick access/logging
    lang_locked = False   # becomes True after first successful detection
    if lang_selected in ("en", "fr", "de"):
        lang_locked = True  # user language already known from earlier detection

    # Replace previous speak_chunk definition with an updated one that
    # does a one-time language detection.
    async def speak_chunk(text_chunk: str):
        nonlocal voice_id, lang_locked, lang_selected
        # --------------------------------------------------------------
        # One-time language detection based on the first chunk
        # --------------------------------------------------------------
        if not lang_locked and text_chunk.strip():
            detected = fast_lang_detect(text_chunk)
            log.info(f"[LANG] Detected {detected.upper()} from first AI chunk â†’ switching voice.")
            lang_selected = detected
            voice_id = VOICE_IDS.get(lang_selected, VOICE_IDS["en"])
            lang_locked = True
        # --------------------------------------------------------------
        # Existing early-exit checks
        # --------------------------------------------------------------
        if call_state["stop_call"] or not text_chunk.strip() or call_state.get("user_is_speaking"):
            log.info(f"[TTS-SPEAK] Skipping speak: stop_call={call_state['stop_call']}, "
                     f"empty_chunk={not text_chunk.strip()}, user_speaking={call_state.get('user_is_speaking')}")
            if call_state.get("user_is_speaking"):
                tts_controller.stop_immediately()
            return False
        # --------------------------------------------------------------
        # Streaming TTS with the (possibly updated) voice_id
        # --------------------------------------------------------------
        log.info(f"[TTS-SPEAK-{stream_sid}] AI âž” {text_chunk[:60].replace(chr(10), ' ')}")
        tts_controller.is_speaking = True
        try:
            tts_controller.current_generator = tts_client.text_to_speech.stream(
                text=text_chunk,
                voice_id=voice_id,
                model_id="eleven_flash_v2_5",
                output_format="ulaw_8000",
                optimize_streaming_latency=0,
            )
            for audio_chunk_count, audio in enumerate(tts_controller.current_generator):
                if call_state.get("user_is_speaking"):
                    log.warning(f"[TTS-CUTOFF-{stream_sid}] User started speaking. Stopping AI TTS.")
                    tts_controller.stop_immediately()
                    return False
                if call_state["stop_call"] or ws.client_state != WebSocketState.CONNECTED:
                    log.warning(f"[TTS-STOP-{stream_sid}] Call stop or WS disconnected. Stopping AI TTS.")
                    tts_controller.stop_immediately()
                    return False
                await ws.send_text(json.dumps({
                    "event": "media",
                    "streamSid": stream_sid,
                    "media": {"payload": base64.b64encode(audio).decode()},
                }))
                # Yield control to the event loop without introducing a fixed delay
                await asyncio.sleep(0)
            return True
        except asyncio.CancelledError:
            log.warning(f"[TTS-CANCELLED-{stream_sid}] TTS task was cancelled.")
            tts_controller.stop_immediately()
            return False
        except Exception as e:
            log.warning(f"[TTS-ERROR-{stream_sid}] TTS stream exception: {e}", exc_info=True)
            tts_controller.stop_immediately()
            return False
        finally:
            pass

    # ------------------------------------------------------------------
    # Build the prompt in the same way as ai_executor.generate_reply but
    # we will *stream* the answer instead of waiting for the entire text.
    # ------------------------------------------------------------------

    MAX_HISTORY_LINES = 100  # Keep in sync with ai_executor
    history_for_prompt = conversation_history[-MAX_HISTORY_LINES:]

    # Build single prompt for the backend
    prompt_for_chat = build_prompt(history_for_prompt, lang_selected)
    # Ask the model to append an explicit action tag so we can avoid a
    # second LLM round-trip.  The tag must be either [[END]] or
    # [[CONTINUE]] and appear **after** the assistant reply.
    prompt_for_chat += (
        "\n\nAt the very end of your reply output exactly the tag [[END]] "
        "if the conversation is finished or [[CONTINUE]] if it should "
        "continue.  Do not output anything after the tag."
    )

    # --------------------------------------------------------------
    # Fetch the full assistant reply in one request (no streaming)
    # --------------------------------------------------------------
    try:
        ai_response_text = await make_openai_request(
            api_key_manager=None,
            model="openchat/openchat-3.5-1210",
            prompt=prompt_for_chat,
            max_tokens=512,
            temperature=0.3,
            top_p=0.95,
        ) or ""
        # ------------------------------------------------------------------
        # Parse the action tag emitted by the model so we know whether to end
        # the call.  Remove the tag from the text that will be synthesised.
        # ------------------------------------------------------------------
        if "[[END]]" in ai_response_text:
            conversation_status = "ended"
            ai_response_text = ai_response_text.replace("[[END]]", "").strip()
        else:
            conversation_status = "continue"
            ai_response_text = ai_response_text.replace("[[CONTINUE]]", "").strip()
        # If the model decided to end, override with a fixed farewell so the
        # caller always hears a concise, branded goodbye.
        if conversation_status == "ended":
            ai_response_text = FAREWELL_LINES.get(lang_selected, FAREWELL_LINES["en"])
    except Exception as exc:
        log.error(f"[AI_TURN-{stream_sid}] Error while generating AI reply: {exc}", exc_info=True)
        ai_response_text = ""
        conversation_status = "continue"

    # Reset accounting for this AI turn
    tts_controller.reset_spoken_text()

    all_spoken_successfully = True

    # --------------------------------------------------------------
    # Speak the reply in CHUNK_WORD_COUNT-word chunks
    # --------------------------------------------------------------
    CHUNK_WORD_COUNT = 100
    words = ai_response_text.split()
    for i in range(0, len(words), CHUNK_WORD_COUNT):
        if call_state["stop_call"]:
            break
        chunk_text = " ".join(words[i : i + CHUNK_WORD_COUNT])
        if not await speak_chunk(chunk_text):
            # The chunk did not finish â€“ mark overall turn as partial
            all_spoken_successfully = False
        else:
            # Entire chunk was played â€“ remember it for later
            tts_controller.add_spoken_text(chunk_text)

    # ------------------------------------------------------------------
    # Append **exactly what was spoken** to the conversation history so the
    # agent remains aware of partial replies when it was interrupted.
    # ------------------------------------------------------------------

    spoken_text = tts_controller.get_spoken_text()

    if spoken_text:
        conversation_history.append(f"AI: {spoken_text}")
    elif ai_response_text:
        # Fallback â€” should only happen when the TTS engine failed before any
        # audio could be delivered.
        conversation_history.append(f"AI: {ai_response_text}")

    # Debug output of updated conversation history
    print(
        f"[{call_state['caller_phone_number']}] HISTORY (now {len(conversation_history)} lines):\n"
        f"{json.dumps(conversation_history, indent=2, ensure_ascii=False)}\n",
        flush=True,
    )

    # No second LLM call needed â€” conversation_status was set above based
    # on the explicit tag returned by the model.  "route" is no longer
    # supported.

    tts_controller.is_speaking = False  # ensure flag reset when turn done / interrupted

    # --------------------------------------------------------------
    # Handle END behaviour (farewell etc.)
    # --------------------------------------------------------------
    if conversation_status == "ended" and all_spoken_successfully:
        log.info(f"[AI_TURN-{stream_sid}] Conversation marked as ended. Terminating after delayâ€¦")
        await asyncio.sleep(END_DELAY_SEC)
        call_state["terminate_call"] = True
        return

    # --------------------------------------------------------------
    # Send Twilio mark if we completed speaking successfully
    # --------------------------------------------------------------
    if all_spoken_successfully and not call_state["stop_call"] and not call_state.get("user_is_speaking") and ws.client_state == WebSocketState.CONNECTED:
        log.info(f"[AI_TURN-MARK-{stream_sid}] Sending end_of_ai_turn mark.")
        try:
            await ws.send_text(json.dumps({"event": "mark", "streamSid": stream_sid,
                                           "mark": {"name": "end_of_ai_turn"}}))
        except RuntimeError as e:
            if "WebSocket is not connected" in str(e) or "after sending 'websocket.close'" in str(e):
                log.warning(f"[AI_TURN-MARK-ERROR-{stream_sid}] WebSocket already closed when trying to send mark: {e}")
            else:
                log.error(f"[AI_TURN-MARK-ERROR-{stream_sid}] Unexpected RuntimeError: {e}", exc_info=True)
        except Exception as e:
            log.error(f"[AI_TURN-MARK-ERROR-{stream_sid}] Failed to send mark: {e}", exc_info=True)
    else:
        log.info(f"[AI_TURN-MARK-SKIP-{stream_sid}] Skipping end_of_ai_turn mark (spoken_successfully={all_spoken_successfully}, stop_call={call_state['stop_call']}, user_speaking={call_state.get('user_is_speaking')}).")

# Helper to detect trivial courtesy phrases that should be ignored

def _is_trivial_thanks(text: str) -> bool:
    txt = text.strip().lower().rstrip(".!?")  # normalize punctuation
    return txt in {"thank you", "you"}